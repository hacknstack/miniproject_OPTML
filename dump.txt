ideas for mini-project in OPT-ML

We have to learn a data-set distributed amongs individuals, problem analysed (from the mini-project description):
data is not iid in each individuals 

using https://arxiv.org/pdf/1912.04977:

(re)call Federated learning, 1 central server, n clients with their own data.

ServerTrains(T):
    init x_0
    for round t=0,..,T-1:
        S_t <- (random set of M clients)
        for client i \in S_t :
            x_(t+1)^i <- ClientUpdate(i,x_t)
        x_(t+1) <- 1/M \sum(k=1)^M x_(t+1)^i 

ClientUpdates(i,x):
    for local steps j = 1,...,K:
        x <- x - gamma gradient(f(x;z)) for z \sim P_i
    return x

(3.1)
theres is several ways (often combined) to have deviation between different clients.
Let's denote the client P_i,P_j for different clients i and j.
P_i(x,y) being not iid P_j(x,y), is better understood with Bayes Theorem:
    P(x,y) = P(y|x) P(x) = P(x|y) P(y)
Thus the 4 cases are (examples are taken from a classification model on number recognition):

    1: Feature distribution shift: P(x) varies across clients, but P(y|x) stays the same
     (they write numbers differently)

    2: Label distribution shift : P(y) varies across clients, but P(x|y) stays the same 
    (people tend to write more 1s and 0s then other numbers)

    3: Concept drift 1: P(x|y) varies across clients but P(y) stays the same:
    (same distribution of numbers but they're wrote very differently across users (cursive, no bar on 7, etc.) )

    4: Concept drift 2: P(y|x) varies across clients but P(x) stays the same:
    (the same ambiguous image might be recognized differently by different people (image between 4 and 9 or 1 and 7) )
There's also a 5th way data can be thought as non-iid; when clients hold vastly different amounts of data.

known strategies available:
    
    1: Optimizations algoritms (3.2.2)
        many found convergence rates depending on the assumption on the non-iid and and function behavior.

    2: Treating each contribution inequally:
        The more the distribution varies from the norm, the less the contribution should be taken into account
        (E.g: multiply by a priority constant, make the selection of clients to train on biased.)
    


Possible Proposition:
    consider an implementation of Federated learning (for example fedAvg) on the mnist dataset,
    We define the base performance on Federated learning with mini-batches GD and iid data distribution in participants.
    How do the performance changes when there is:
        - Feature distribution shift
        - Label distribution shift
    For each of them we can try solution where each clients is weighted by how much the distribution varies, many solutions:
        - KL-divergence D(P||Q) = \sum_x P(x) log(P(x)/Q(x))
        - Jensen-Shanon divergence (the symetric alternative) JSD(P||Q) = (D(P||Q)+D(Q||P))/2
        - one final that is quite different from theses 2.

------------------------------------------------------------------------------------------------------

Observation:
    the EMNIST dataset has 4099 ~10^3 different authors.
    240000 ~10^5, which means 60 ~10^2 numbers per author.

For label distribution shift : (introduces in B. from https://arxiv.org/pdf/2102.02079)
two different ways to model it:
    -quantity-based:
        each client has "only" k different labels (k<10) the k labels are choosen at random for each client
    -distribution-based :
        each client has a proportion of each label according to a Dirichlet distribution (for recall https://www.youtube.com/watch?v=_e0ugNro0Rc)
        we sample p_k from Dir_10(beta)=[p0,...,p9] for each class, for more skewed distribution, make B ->0.

For a more realistic and nuanced approach let's go for the distribution-based label distribution shift model!

For Feature distribution shift: (introduces in C. from https://arxiv.org/pdf/2102.02079)
There are multiple ways to aproach this problem. But in our case we already have a natural feature distribution shift, since each writer writes differently
so we apply the Real world feature imbalance:
    each client receives k random different writers