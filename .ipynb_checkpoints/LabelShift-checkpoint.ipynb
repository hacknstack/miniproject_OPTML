{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ff48ec6a-8760-4059-93d8-3941b871a991",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "from utils import *\n",
    "data = loadmat('emnist-digits.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "00905d30-b071-4b7d-99b1-64266e8e742b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "38e6610d-be6a-4158-86a4-1d35b573b187",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data['dataset'][0, 0]\n",
    "train = dataset[0][0, 0]  \n",
    "test = dataset[1][0, 0]  \n",
    "mapping = dataset[2]\n",
    "\n",
    "train_images = train['images']   # Shape: (N, 28*28)\n",
    "train_labels = train['labels']  # Shape: (N, 1)\n",
    "train_writers = train['writers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4714ba0d-3b54-4b1a-9723-3cdd6e691cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_images.astype(np.float32) / 255.0  # Normalize to [0, 1]\n",
    "y = train_labels.flatten().astype(np.int64)\n",
    "\n",
    "# Wrap into a datalist with a single client\n",
    "datalist = [(X, y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f366af4a-1dcc-4073-8f32-89e997c7d80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now training the baseline, i.e. fedAvg with one client holding all the data\n",
      "round :  1\n",
      "round :  2\n",
      "round :  3\n",
      "round :  4\n",
      "round :  5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Hyperparameters\n",
    "T = 5       # number of global rounds\n",
    "K = 10      # number of client GD steps\n",
    "gamma = 0.1 # learning rate\n",
    "\n",
    "# Run FedAvg with 1 client\n",
    "print(\"now training the baseline, i.e. fedAvg with one client holding all the data\")\n",
    "model = fedavg(datalist, T, K, gamma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "93104499-2f8e-47ca-a101-6d8481a0a12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 84.37%\n"
     ]
    }
   ],
   "source": [
    "test_images = test['images'].astype(np.float32) / 255.0\n",
    "test_labels = test['labels'].flatten().astype(np.int64)\n",
    "\n",
    "test_accuracy = evaluate(model, test_images, test_labels)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "553cc35f-7e84-4377-86af-e47023832dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case with 5 clients, beta=0.5 skewed distribution!\n",
      "round :  1\n",
      "round :  2\n",
      "round :  3\n",
      "round :  4\n",
      "round :  5\n"
     ]
    }
   ],
   "source": [
    "n_clients = 5\n",
    "beta = 0.5 \n",
    "datalist = create_dirichlet_clients(X, y, n_clients, beta)\n",
    "\n",
    "# Hyperparameters\n",
    "T = 5       # number of global rounds\n",
    "K = 10      # number of client GD steps\n",
    "gamma = 0.1 # learning rate\n",
    "print(\"case with 5 clients, beta=0.5 skewed distribution!\")\n",
    "model = fedavg(datalist, T, K, gamma)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c9b636ec-9a2a-4b33-847c-e4926530ea4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy with 5 clients and Dir(0.5): 80.88%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "test_accuracy = evaluate(model, test_images, test_labels)\n",
    "print(f\"Test Accuracy with {n_clients} clients and Dir({beta}): {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "61859fa6-aa3d-46ad-84a3-815b447f05d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case with 5 clients, beta=10^5, which means close to IID clients (baseline 2)!\n",
      "round :  1\n",
      "round :  2\n",
      "round :  3\n",
      "round :  4\n",
      "round :  5\n"
     ]
    }
   ],
   "source": [
    "n_clients = 5\n",
    "beta = 1e5\n",
    "datalist = create_dirichlet_clients(X, y, n_clients, beta)\n",
    "\n",
    "# Hyperparameters\n",
    "T = 5       # number of global rounds\n",
    "K = 10      # number of client GD steps\n",
    "gamma = 0.1 # learning rate\n",
    "print(\"case with 5 clients, beta=10^5, which means close to IID clients (baseline 2)!\")\n",
    "model = fedavg(datalist, T, K, gamma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7956a33e-da06-4fb5-a6cf-c5b5db3fe46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy with 5 clients and Dir(100000.0): 84.56%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "test_accuracy = evaluate(model, test_images, test_labels)\n",
    "print(f\"Test Accuracy with {n_clients} clients and Dir({beta}): {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "18132c86-be68-4d4f-bc45-541b0c0622e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "def make_femnist_datasets(X,y, K=10, seed=42):\n",
    "\n",
    "    # 1) Group example‐indices by writer\n",
    "    by_writer = defaultdict(list)\n",
    "    for idx, writer in enumerate(train['writers']):\n",
    "        by_writer[int(writer)].append(idx)\n",
    "\n",
    "    # 2) shuffle writer IDs and split into K groups\n",
    "    writer_ids = list(by_writer.keys())\n",
    "    random.seed(seed)\n",
    "    random.shuffle(writer_ids)\n",
    "    per = len(writer_ids) // K\n",
    "    groups = [writer_ids[i*per : (i+1)*per] for i in range(K-1)]\n",
    "    groups.append(writer_ids[(K-1)*per :])\n",
    "\n",
    "    # 3) for each group, collect X_i, y_i\n",
    "    datalist = []\n",
    "    for group in groups:\n",
    "        idxs = [i for w in group for i in by_writer[w]]\n",
    "        Xi = X[idxs]   # shape [n_i, ...]\n",
    "        yi = y[idxs]   # shape [n_i,]\n",
    "        datalist.append((Xi, yi))\n",
    "\n",
    "    return datalist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2101a5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_11352\\4229544894.py:9: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  by_writer[int(writer)].append(idx)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case with 30 clients, with feature distribution shift\n",
      "round :  1\n",
      "round :  2\n",
      "round :  3\n",
      "round :  4\n",
      "round :  5\n"
     ]
    }
   ],
   "source": [
    "n_clients = 30\n",
    "datalist = make_femnist_datasets(X,y,n_clients)\n",
    "# Hyperparameters\n",
    "T = 5       # number of global rounds\n",
    "K = 10      # number of client GD steps\n",
    "gamma = 0.1 # learning rate\n",
    "print(f\"case with {n_clients} clients, with feature distribution shift\")\n",
    "model = fedavg(datalist, T, K, gamma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "efc52e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy with 30 ): 84.19%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "test_accuracy = evaluate(model, test_images, test_labels)\n",
    "print(f\"Test Accuracy with {n_clients} ): {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "653f31ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MADE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(MADE, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, input_dim)\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.fc2(F.relu(self.fc1(x))))\n",
    "\n",
    "\n",
    "\n",
    "class WeightEstimator(nn.Module):\n",
    "    \"\"\"\n",
    "    Estimates the weight α(x) = P(l=1 | u) / (1 - P(l=1 | u))\n",
    "    based on MADE log-likelihood vectors.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim=100):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.relu(self.fc1(x))\n",
    "        return self.sigmoid(self.fc2(h)).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cdfa51ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "def train_local_made(model, loader, epochs=5, lr=1e-3):\n",
    "    opt = optim.Adam(model.parameters(), lr=lr)\n",
    "    for _ in range(epochs):\n",
    "        for x, _ in loader:\n",
    "            out = model(x)\n",
    "            loss = F.binary_cross_entropy(out, x)\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "    return model.state_dict()\n",
    "\n",
    "def aggregate_models(states, weights):\n",
    "    return {k: sum(weights[i] * states[i][k] for i in range(len(states)))\n",
    "            for k in states[0]}\n",
    "\n",
    "def train_global_made(loaders, dim, hid, rounds=10, local_epochs=1):\n",
    "    gm = MADE(dim, hid)\n",
    "    for _ in range(rounds):\n",
    "        states, sizes = [], []\n",
    "        for ld in loaders:\n",
    "            lm = MADE(dim, hid)\n",
    "            lm.load_state_dict(gm.state_dict())\n",
    "            sd = train_local_made(lm, ld, epochs=local_epochs)\n",
    "            states.append(sd); sizes.append(len(ld.dataset))\n",
    "        total = sum(sizes)\n",
    "        gm.load_state_dict(aggregate_models(states, [s/total for s in sizes]))\n",
    "    return gm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fca98f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.bernoulli import Bernoulli\n",
    "\n",
    "\n",
    "def compute_sample_weights(global_made, local_made, loader,\n",
    "                           device='cpu', num_epochs=1, lr=1e-3):\n",
    "    \"\"\"\n",
    "    Trains the WeightEstimator to distinguish global vs local MADE log-likelihoods\n",
    "    and computes sample weights α for all samples in loader.\n",
    "\n",
    "    Args:\n",
    "        global_made, local_made: MADE models; their forward(x) returns logits for Bernoulli outputs.\n",
    "        loader: DataLoader yielding (X, _) batches (X in [0,1]).\n",
    "        device: 'cpu' or 'cuda'.\n",
    "        num_epochs: number of training epochs for estimator.\n",
    "        lr: learning rate.\n",
    "\n",
    "    Returns:\n",
    "        Tensor of α weights for all samples in order.\n",
    "    \"\"\"\n",
    "    # Determine input dimension\n",
    "    sample_batch = next(iter(loader))[0].to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = global_made(sample_batch)\n",
    "    input_dim = logits.size(1)\n",
    "\n",
    "    estimator = WeightEstimator(input_dim).to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(estimator.parameters(), lr=lr)\n",
    "\n",
    "    # Train\n",
    "    estimator.train()\n",
    "    for _ in range(num_epochs):\n",
    "        for X, _ in loader:\n",
    "            X = X.to(device)\n",
    "            X_bin = (X >= 0.5).float()\n",
    "            with torch.no_grad():\n",
    "                ug = Bernoulli(logits=global_made(X)).log_prob(X_bin)\n",
    "                ul = Bernoulli(logits=local_made(X)).log_prob(X_bin)\n",
    "            U = torch.cat([ug, ul], dim=0)\n",
    "            labels = torch.cat([\n",
    "                torch.zeros(ug.size(0)),\n",
    "                torch.ones(ul.size(0))\n",
    "            ]).to(device)\n",
    "            preds = estimator(U)\n",
    "            loss = criterion(preds, labels)\n",
    "            optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "\n",
    "    # Compute α for each sample\n",
    "    estimator.eval()\n",
    "    alphas = []\n",
    "    with torch.no_grad():\n",
    "        for X, _ in loader:\n",
    "            X = X.to(device)\n",
    "            X_bin = (X >= 0.5).float()\n",
    "            ul = Bernoulli(logits=local_made(X)).log_prob(X_bin)\n",
    "            p = estimator(ul)\n",
    "            alphas.append((p / (1 - p)).cpu())\n",
    "    return torch.cat(alphas)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "91284664",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_11352\\4229544894.py:9: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  by_writer[int(writer)].append(idx)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 27\u001b[0m\n\u001b[0;32m     14\u001b[0m made_loaders \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     15\u001b[0m     DataLoader(\n\u001b[0;32m     16\u001b[0m         TensorDataset(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m X, _ \u001b[38;5;129;01min\u001b[39;00m datalist\n\u001b[0;32m     24\u001b[0m ]\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# 2) Train global MADE\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m global_made \u001b[38;5;241m=\u001b[39m train_global_made(\n\u001b[0;32m     28\u001b[0m     made_loaders,\n\u001b[0;32m     29\u001b[0m     dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m28\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m28\u001b[39m,\n\u001b[0;32m     30\u001b[0m     hid\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[0;32m     31\u001b[0m     rounds\u001b[38;5;241m=\u001b[39mT,\n\u001b[0;32m     32\u001b[0m     local_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     33\u001b[0m )\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# 3) Compute sample‐weights α for each client\u001b[39;00m\n\u001b[0;32m     36\u001b[0m sample_weights \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[1;32mIn[33], line 25\u001b[0m, in \u001b[0;36mtrain_global_made\u001b[1;34m(loaders, dim, hid, rounds, local_epochs)\u001b[0m\n\u001b[0;32m     23\u001b[0m     lm \u001b[38;5;241m=\u001b[39m MADE(dim, hid)\n\u001b[0;32m     24\u001b[0m     lm\u001b[38;5;241m.\u001b[39mload_state_dict(gm\u001b[38;5;241m.\u001b[39mstate_dict())\n\u001b[1;32m---> 25\u001b[0m     sd \u001b[38;5;241m=\u001b[39m train_local_made(lm, ld, epochs\u001b[38;5;241m=\u001b[39mlocal_epochs)\n\u001b[0;32m     26\u001b[0m     states\u001b[38;5;241m.\u001b[39mappend(sd); sizes\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mlen\u001b[39m(ld\u001b[38;5;241m.\u001b[39mdataset))\n\u001b[0;32m     27\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(sizes)\n",
      "Cell \u001b[1;32mIn[33], line 11\u001b[0m, in \u001b[0;36mtrain_local_made\u001b[1;34m(model, loader, epochs, lr)\u001b[0m\n\u001b[0;32m      9\u001b[0m         out \u001b[38;5;241m=\u001b[39m model(x)\n\u001b[0;32m     10\u001b[0m         loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mbinary_cross_entropy(out, x)\n\u001b[1;32m---> 11\u001b[0m         opt\u001b[38;5;241m.\u001b[39mzero_grad(); loss\u001b[38;5;241m.\u001b[39mbackward(); opt\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\u001b[38;5;241m.\u001b[39mstate_dict()\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\optim\\optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    485\u001b[0m             )\n\u001b[1;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\optim\\optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\optim\\adam.py:223\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    211\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    213\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    214\u001b[0m         group,\n\u001b[0;32m    215\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    220\u001b[0m         state_steps,\n\u001b[0;32m    221\u001b[0m     )\n\u001b[1;32m--> 223\u001b[0m     adam(\n\u001b[0;32m    224\u001b[0m         params_with_grad,\n\u001b[0;32m    225\u001b[0m         grads,\n\u001b[0;32m    226\u001b[0m         exp_avgs,\n\u001b[0;32m    227\u001b[0m         exp_avg_sqs,\n\u001b[0;32m    228\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    229\u001b[0m         state_steps,\n\u001b[0;32m    230\u001b[0m         amsgrad\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mamsgrad\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    231\u001b[0m         has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[0;32m    232\u001b[0m         beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[0;32m    233\u001b[0m         beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[0;32m    234\u001b[0m         lr\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    235\u001b[0m         weight_decay\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    236\u001b[0m         eps\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    237\u001b[0m         maximize\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    238\u001b[0m         foreach\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforeach\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    239\u001b[0m         capturable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcapturable\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    240\u001b[0m         differentiable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    241\u001b[0m         fused\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfused\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    242\u001b[0m         grad_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_scale\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    243\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    244\u001b[0m     )\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\optim\\optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\optim\\adam.py:784\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    781\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    782\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 784\u001b[0m func(\n\u001b[0;32m    785\u001b[0m     params,\n\u001b[0;32m    786\u001b[0m     grads,\n\u001b[0;32m    787\u001b[0m     exp_avgs,\n\u001b[0;32m    788\u001b[0m     exp_avg_sqs,\n\u001b[0;32m    789\u001b[0m     max_exp_avg_sqs,\n\u001b[0;32m    790\u001b[0m     state_steps,\n\u001b[0;32m    791\u001b[0m     amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[0;32m    792\u001b[0m     has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[0;32m    793\u001b[0m     beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[0;32m    794\u001b[0m     beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[0;32m    795\u001b[0m     lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[0;32m    796\u001b[0m     weight_decay\u001b[38;5;241m=\u001b[39mweight_decay,\n\u001b[0;32m    797\u001b[0m     eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[0;32m    798\u001b[0m     maximize\u001b[38;5;241m=\u001b[39mmaximize,\n\u001b[0;32m    799\u001b[0m     capturable\u001b[38;5;241m=\u001b[39mcapturable,\n\u001b[0;32m    800\u001b[0m     differentiable\u001b[38;5;241m=\u001b[39mdifferentiable,\n\u001b[0;32m    801\u001b[0m     grad_scale\u001b[38;5;241m=\u001b[39mgrad_scale,\n\u001b[0;32m    802\u001b[0m     found_inf\u001b[38;5;241m=\u001b[39mfound_inf,\n\u001b[0;32m    803\u001b[0m )\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\optim\\adam.py:369\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight_decay \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    367\u001b[0m     grad \u001b[38;5;241m=\u001b[39m grad\u001b[38;5;241m.\u001b[39madd(param, alpha\u001b[38;5;241m=\u001b[39mweight_decay)\n\u001b[1;32m--> 369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(param):\n\u001b[0;32m    370\u001b[0m     grad \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_real(grad)\n\u001b[0;32m    371\u001b[0m     exp_avg \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_real(exp_avg)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "n_clients = 30\n",
    "datalist = make_femnist_datasets(X,y,n_clients)\n",
    "\n",
    "# Hyperparameters\n",
    "T = 5       # number of global rounds\n",
    "K = 10      # number of client GD steps\n",
    "gamma = 0.1 # learning rate\n",
    "\n",
    "# 1) Create MADE data loaders\n",
    "made_loaders = [\n",
    "    DataLoader(\n",
    "        TensorDataset(\n",
    "            torch.tensor(X, dtype=torch.float32),\n",
    "            torch.zeros(len(X), dtype=torch.float32)\n",
    "        ),\n",
    "        batch_size=64,\n",
    "        shuffle=True\n",
    "    )\n",
    "    for X, _ in datalist\n",
    "]\n",
    "\n",
    "# 2) Train global MADE\n",
    "global_made = train_global_made(\n",
    "    made_loaders,\n",
    "    dim=28*28,\n",
    "    hid=100,\n",
    "    rounds=T,\n",
    "    local_epochs=1\n",
    ")\n",
    "\n",
    "# 3) Compute sample‐weights α for each client\n",
    "sample_weights = []\n",
    "for ld in made_loaders:\n",
    "    # train local MADE and load its weights\n",
    "    local_made = MADE(28*28, 100)\n",
    "    local_state = train_local_made(local_made, ld, epochs=1)\n",
    "    local_made.load_state_dict(local_state)\n",
    "\n",
    "    # compute α for this client (Tensor of shape [n_samples])\n",
    "    alpha = compute_sample_weights(global_made, local_made, ld)\n",
    "    sample_weights.append(alpha)\n",
    "\n",
    "# 4) Build weighted datasets by oversampling\n",
    "weighted_datalist = []\n",
    "for (Xi, yi), a in zip(datalist, sample_weights):\n",
    "    # normalize and scale to counts, then convert to numpy ints\n",
    "    counts = (a / a.sum() * len(yi)).cpu().numpy().astype(int)\n",
    "    idxs = np.repeat(np.arange(len(yi)), counts)\n",
    "    weighted_datalist.append((Xi[idxs], yi[idxs]))\n",
    "\n",
    "# 5) Federated training on weighted data\n",
    "print(f\"case with {n_clients} clients, with feature distribution shift\")\n",
    "model = fedavg(weighted_datalist, T, K, gamma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592b7450",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_accuracy = evaluate(model, test_images, test_labels)\n",
    "print(f\"Test Accuracy with {n_clients} ): {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4649ec-6344-49e0-89bb-6535c9c35d14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
